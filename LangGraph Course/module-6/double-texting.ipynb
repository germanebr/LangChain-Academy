{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double Texting\n",
    "\n",
    "Seamless handling of [double texting](https://langchain-ai.github.io/langgraph/concepts/double_texting/) is important for handling real-world usage scenarios, especially in chat applications.\n",
    "\n",
    "Users can send multiple messages in a row before the prior run(s) complete, and we want to ensure that we handle this gracefully.\n",
    "\n",
    "## Reject\n",
    "\n",
    "A simple approach is to [reject](https://langchain-ai.github.io/langgraph/cloud/how-tos/reject_concurrent/) any new runs until the current run completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph_sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client\n",
    "url_for_cli_deployment = \"http://localhost:8123\"\n",
    "client = get_client(url=url_for_cli_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to start concurrent run Client error '409 Conflict' for url 'http://localhost:8123/threads/7d3cad29-105e-42dc-984e-54e8b7de9c1d/runs'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/409\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Create a thread\n",
    "thread = await client.threads.create()\n",
    "\n",
    "# Create to dos\n",
    "user_input_1 = \"Add a ToDo to follow-up with DI Repairs.\"\n",
    "user_input_2 = \"Add a ToDo to mount dresser to the wall.\"\n",
    "config = {\"configurable\": {\"user_id\": \"Lance\"}}\n",
    "graph_name = \"task_maistro\" \n",
    "\n",
    "run = await client.runs.create(\n",
    "    thread[\"thread_id\"],\n",
    "    graph_name,\n",
    "    input={\"messages\": [HumanMessage(content=user_input_1)]}, \n",
    "    config=config,\n",
    ")\n",
    "try:\n",
    "    await client.runs.create(\n",
    "        thread[\"thread_id\"],\n",
    "        graph_name,\n",
    "        input={\"messages\": [HumanMessage(content=user_input_2)]}, \n",
    "        config=config,\n",
    "        multitask_strategy=\"reject\",\n",
    "    )\n",
    "except httpx.HTTPStatusError as e:\n",
    "    print(\"Failed to start concurrent run\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Add a ToDo to follow-up with DI Repairs.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "It looks like you already have a task to follow-up with DI Repairs on your ToDo list. Would you like to update the status or details of that task instead?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "# Wait until the original run completes\n",
    "await client.runs.join(thread[\"thread_id\"], run[\"run_id\"])\n",
    "\n",
    "# Get the state of the thread\n",
    "state = await client.threads.get_state(thread[\"thread_id\"])\n",
    "for m in convert_to_messages(state[\"values\"][\"messages\"]):\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enqueue\n",
    "\n",
    "We can use [enqueue](https://langchain-ai.github.io/langgraph/cloud/how-tos/enqueue_concurrent/https://langchain-ai.github.io/langgraph/cloud/how-tos/enqueue_concurrent/) any new runs until the current run completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Send Erik his t-shirt gift this weekend.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UpdateMemory (call_qXDVcN1zjyQUj4aRNlQ3BQOq)\n",
      " Call ID: call_qXDVcN1zjyQUj4aRNlQ3BQOq\n",
      "  Args:\n",
      "    update_type: todo\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Document 45114b64-a6b7-4d24-bbad-b339da309224 updated:\n",
      "Plan: Update the task to send Erik his t-shirt gift this weekend, and set the status to 'not started'.\n",
      "Added content: Send Erik his t-shirt gift this weekend.\n",
      "\n",
      "New ToDo created:\n",
      "Content: {'task': 'Send Erik his t-shirt gift this weekend.', 'time_to_complete': 15, 'deadline': '2024-01-06T00:00:00Z', 'status': 'not started'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've added the task to send Erik his t-shirt gift this weekend to your ToDo list. If you need any help with it, just let me know!\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Get cash and pay nanny for 2 weeks. Do this by Friday.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UpdateMemory (call_ngxbcJQlJocve9ma30xxEWIp)\n",
      " Call ID: call_ngxbcJQlJocve9ma30xxEWIp\n",
      "  Args:\n",
      "    update_type: todo\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "New ToDo created:\n",
      "Content: {'task': 'Get cash and pay nanny for 2 weeks.', 'time_to_complete': 30, 'deadline': '2024-01-05T00:00:00Z', 'status': 'not started'}\n",
      "\n",
      "Document 45114b64-a6b7-4d24-bbad-b339da309224 updated:\n",
      "Plan: Update the task to send Erik his t-shirt gift this weekend, and set the status to 'not started'.\n",
      "Added content: Send Erik his t-shirt gift this weekend.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've added the task to get cash and pay the nanny for 2 weeks, with a deadline of Friday. If there's anything else you need, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "# Create a new thread\n",
    "thread = await client.threads.create()\n",
    "\n",
    "# Create new ToDos\n",
    "user_input_1 = \"Send Erik his t-shirt gift this weekend.\"\n",
    "user_input_2 = \"Get cash and pay nanny for 2 weeks. Do this by Friday.\"\n",
    "config = {\"configurable\": {\"user_id\": \"Lance\"}}\n",
    "graph_name = \"task_maistro\" \n",
    "\n",
    "first_run = await client.runs.create(\n",
    "    thread[\"thread_id\"],\n",
    "    graph_name,\n",
    "    input={\"messages\": [HumanMessage(content=user_input_1)]}, \n",
    "    config=config,\n",
    ")\n",
    "\n",
    "second_run = await client.runs.create(\n",
    "    thread[\"thread_id\"],\n",
    "    graph_name,\n",
    "    input={\"messages\": [HumanMessage(content=user_input_2)]}, \n",
    "    config=config,\n",
    "    multitask_strategy=\"enqueue\",\n",
    ")\n",
    "\n",
    "# Wait until the second run completes\n",
    "await client.runs.join(thread[\"thread_id\"], second_run[\"run_id\"])\n",
    "\n",
    "# Get the state of the thread\n",
    "state = await client.threads.get_state(thread[\"thread_id\"])\n",
    "for m in convert_to_messages(state[\"values\"][\"messages\"]):\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interrupt\n",
    "\n",
    "We can use [interrupt](https://langchain-ai.github.io/langgraph/cloud/how-tos/interrupt_concurrent/) to interrupt the current run, but save all the work that has been done so far up to that point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Never mind, Thanksgiving is the 28th! Order Ham for Thanksgiving by next Friday.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UpdateMemory (call_JXHFQRMuRzTMjw3KRs9sR3I0)\n",
      " Call ID: call_JXHFQRMuRzTMjw3KRs9sR3I0\n",
      "  Args:\n",
      "    update_type: todo\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "New ToDo created:\n",
      "Content: {'task': 'Order Ham for Thanksgiving', 'time_to_complete': 30, 'deadline': '2024-01-05T00:00:00Z', 'solutions': ['Check local grocery stores for ham availability.', 'Consider ordering online for delivery.'], 'status': 'not started'}\n",
      "\n",
      "Document 45114b64-a6b7-4d24-bbad-b339da309224 updated:\n",
      "Plan: Update the task to reflect the new deadline and status.\n",
      "Added content: Send Erik his t-shirt gift this weekend.\n",
      "\n",
      "Document c3ee571a-321d-4bca-a69a-6f49b273ae54 updated:\n",
      "Plan: Update the task to reflect the new deadline and status.\n",
      "Added content: Get cash and pay nanny for 2 weeks.\n",
      "\n",
      "Document c3caf704-c4bd-4da8-a04e-113efdae65ae updated:\n",
      "Plan: Update the task to reflect the new deadline and status.\n",
      "Added content: Send Erik his t-shirt gift this weekend.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've updated your ToDo list to reflect that you need to order ham for Thanksgiving by next Friday. If you need any more help or have other tasks to add, just let me know!\n"
     ]
    }
   ],
   "source": [
    "# Create a new thread\n",
    "thread = await client.threads.create()\n",
    "\n",
    "# Create new ToDos\n",
    "user_input_1 = \"Order turkey for Thanksgiving by Friday.\"\n",
    "user_input_2 = \"Never mind, Thanksgiving is the 28th! Order Ham for Thanksgiving by next Friday.\"\n",
    "config = {\"configurable\": {\"user_id\": \"Lance\"}}\n",
    "graph_name = \"task_maistro\" \n",
    "\n",
    "interrupted_run = await client.runs.create(\n",
    "    thread[\"thread_id\"],\n",
    "    graph_name,\n",
    "    input={\"messages\": [HumanMessage(content=user_input_1)]}, \n",
    "    config=config,\n",
    ")\n",
    "\n",
    "second_run = await client.runs.create(\n",
    "    thread[\"thread_id\"],\n",
    "    graph_name,\n",
    "    input={\"messages\": [HumanMessage(content=user_input_2)]}, \n",
    "    config=config,\n",
    "    multitask_strategy=\"interrupt\",\n",
    ")\n",
    "\n",
    "# Wait until the second run completes\n",
    "await client.runs.join(thread[\"thread_id\"], second_run[\"run_id\"])\n",
    "\n",
    "# Get the state of the thread\n",
    "state = await client.threads.get_state(thread[\"thread_id\"])\n",
    "for m in convert_to_messages(state[\"values\"][\"messages\"]):\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "interrupted\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the first run was interrupted\n",
    "print((await client.runs.get(thread[\"thread_id\"], interrupted_run[\"run_id\"]))[\"status\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollback\n",
    "\n",
    "We can use [rollback](https://langchain-ai.github.io/langgraph/cloud/how-tos/rollback_concurrent/) to interrupt the prior run of the graph and starts a new one with the double-texted input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Actually, add a ToDo to drop by Yoga in person on Sunday.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  UpdateMemory (call_Ff9FptNtfXv5bPunXzeOi8jz)\n",
      " Call ID: call_Ff9FptNtfXv5bPunXzeOi8jz\n",
      "  Args:\n",
      "    update_type: todo\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "New ToDo created:\n",
      "Content: {'task': 'Drop by Yoga in person on Sunday.', 'time_to_complete': 60, 'deadline': '2024-01-07T00:00:00Z', 'solutions': ['Check class schedule for Sunday.', 'Bring yoga mat and water bottle.'], 'status': 'not started'}\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I've added the task to drop by Yoga in person on Sunday to your ToDo list. If you need anything else, just let me know!\n"
     ]
    }
   ],
   "source": [
    "# Create a new thread\n",
    "thread = await client.threads.create()\n",
    "\n",
    "# Create new ToDos\n",
    "user_input_1 = \"Add a ToDo to call to make appointment at Yoga.\"\n",
    "user_input_2 = \"Actually, add a ToDo to drop by Yoga in person on Sunday.\"\n",
    "config = {\"configurable\": {\"user_id\": \"Lance\"}}\n",
    "graph_name = \"task_maistro\" \n",
    "\n",
    "rolled_back_run = await client.runs.create(\n",
    "    thread[\"thread_id\"],\n",
    "    graph_name,\n",
    "    input={\"messages\": [HumanMessage(content=user_input_1)]}, \n",
    "    config=config,\n",
    ")\n",
    "\n",
    "second_run = await client.runs.create(\n",
    "    thread[\"thread_id\"],\n",
    "    graph_name,\n",
    "    input={\"messages\": [HumanMessage(content=user_input_2)]}, \n",
    "    config=config,\n",
    "    multitask_strategy=\"rollback\",\n",
    ")\n",
    "\n",
    "# Wait until the second run completes\n",
    "await client.runs.join(thread[\"thread_id\"], second_run[\"run_id\"])\n",
    "\n",
    "# Get the state of the thread\n",
    "state = await client.threads.get_state(thread[\"thread_id\"])\n",
    "for m in convert_to_messages(state[\"values\"][\"messages\"]):\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original run was correctly deleted\n"
     ]
    }
   ],
   "source": [
    "# Confirm that the original run was deleted\n",
    "try:\n",
    "    await client.runs.get(thread[\"thread_id\"], rolled_back_run[\"run_id\"])\n",
    "except httpx.HTTPStatusError as _:\n",
    "    print(\"Original run was correctly deleted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPStatusError",
     "evalue": "Client error '404 Not Found' for url 'http://localhost:8123/threads/985890f6-6b19-42cd-81be-a86bff91037a/runs/1efc6fa3-d8ab-6c42-a25e-12509d065179'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mruns\u001b[38;5;241m.\u001b[39mget(thread[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m], rolled_back_run[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      2\u001b[0m state\n",
      "File \u001b[1;32mc:\\Users\\Germ치nEduardoBaltaza\\Desktop\\LangChain-Academy\\.venv\\Lib\\site-packages\\langgraph_sdk\\client.py:1768\u001b[0m, in \u001b[0;36mRunsClient.get\u001b[1;34m(self, thread_id, run_id)\u001b[0m\n\u001b[0;32m   1749\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, thread_id: \u001b[38;5;28mstr\u001b[39m, run_id: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Run:\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a run.\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m \n\u001b[0;32m   1752\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1765\u001b[0m \n\u001b[0;32m   1766\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[1;32m-> 1768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/threads/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mthread_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/runs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Germ치nEduardoBaltaza\\Desktop\\LangChain-Academy\\.venv\\Lib\\site-packages\\langgraph_sdk\\client.py:216\u001b[0m, in \u001b[0;36mHttpClient.get\u001b[1;34m(self, path, params)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m         logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError from langgraph-api: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbody\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m adecode_json(r)\n",
      "File \u001b[1;32mc:\\Users\\Germ치nEduardoBaltaza\\Desktop\\LangChain-Academy\\.venv\\Lib\\site-packages\\langgraph_sdk\\client.py:209\u001b[0m, in \u001b[0;36mHttpClient.get\u001b[1;34m(self, path, params)\u001b[0m\n\u001b[0;32m    207\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mget(path, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 209\u001b[0m     \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mHTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    211\u001b[0m     body \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mawait\u001b[39;00m r\u001b[38;5;241m.\u001b[39maread())\u001b[38;5;241m.\u001b[39mdecode()\n",
      "File \u001b[1;32mc:\\Users\\Germ치nEduardoBaltaza\\Desktop\\LangChain-Academy\\.venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    827\u001b[0m error_type \u001b[38;5;241m=\u001b[39m error_types\u001b[38;5;241m.\u001b[39mget(status_class, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid status code\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    828\u001b[0m message \u001b[38;5;241m=\u001b[39m message\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m, error_type\u001b[38;5;241m=\u001b[39merror_type)\n\u001b[1;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request\u001b[38;5;241m=\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPStatusError\u001b[0m: Client error '404 Not Found' for url 'http://localhost:8123/threads/985890f6-6b19-42cd-81be-a86bff91037a/runs/1efc6fa3-d8ab-6c42-a25e-12509d065179'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404",
      "\u001b[0mNot Found"
     ]
    }
   ],
   "source": [
    "state = await client.runs.get(thread[\"thread_id\"], rolled_back_run[\"run_id\"])\n",
    "state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
